{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"Kopie von demo.ipynb","provenance":[{"file_id":"https://github.com/segments-ai/fast-labeling-workflow/blob/master/demo.ipynb","timestamp":1608672354011}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FtEXRZACV7m1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608680774334,"user_tz":-60,"elapsed":9347,"user":{"displayName":"Nico","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0Bb0bACIyv9fqV3zWzGefWKcxTlKtkEba4qlPeA=s64","userId":"17943449425803794492"}},"outputId":"61ccbbf2-359a-4451-a4d5-afe68a367ea3"},"source":["#####################################################################################################\n","##  Uncomment following lines when running in Google Colab, to install the required dependencies.  ##\n","#####################################################################################################\n","\n","import torch\n","assert torch.cuda.is_available(), 'You need a GPU! In Colab, go to Runtime -> Change runtime type -> Hardware accelerator -> GPU'\n","\n","!pip install segments-ai\n","!pip install pyyaml==5.1\n","!pip install torch==1.7\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n","#!git clone https://github.com/segments-ai/fast-labeling-workflow\n","%cd fast-labeling-workflow"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: segments-ai in /usr/local/lib/python3.6/dist-packages (0.22)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from segments-ai) (2.23.0)\n","Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from segments-ai) (2.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from segments-ai) (1.19.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from segments-ai) (8.0.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from segments-ai) (4.41.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->segments-ai) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->segments-ai) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->segments-ai) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->segments-ai) (3.0.4)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools->segments-ai) (0.29.21)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->segments-ai) (3.2.2)\n","Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools->segments-ai) (51.0.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools->segments-ai) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools->segments-ai) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools->segments-ai) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.0->pycocotools->segments-ai) (2.4.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools->segments-ai) (1.15.0)\n","Requirement already satisfied: pyyaml==5.1 in /usr/local/lib/python3.6/dist-packages (5.1)\n","Requirement already satisfied: torch==1.7 in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (1.19.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7) (0.8)\n","Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n","Requirement already satisfied: detectron2 in /usr/local/lib/python3.6/dist-packages (0.3+cu101)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.8.7)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from detectron2) (2.4.0)\n","Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from detectron2) (2.0.2)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.3.0)\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.6/dist-packages (from detectron2) (8.0.1)\n","Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.1.8)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from detectron2) (1.1.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from detectron2) (3.2.2)\n","Requirement already satisfied: fvcore>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.1.2.post20201218)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from detectron2) (0.16.0)\n","Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.6/dist-packages (from detectron2) (4.41.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (2.23.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.7.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (51.0.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.36.2)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.12.4)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.32.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.10.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (3.3.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.17.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.15.0)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (1.19.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->detectron2) (0.4.2)\n","Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from pycocotools>=2.0.2->detectron2) (0.29.21)\n","Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot->detectron2) (2.4.7)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from yacs>=0.1.6->detectron2) (5.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->detectron2) (1.3.1)\n","Requirement already satisfied: iopath>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from fvcore>=0.1.2->detectron2) (0.1.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.0.4)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->detectron2) (3.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.2.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from iopath>=0.1.2->fvcore>=0.1.2->detectron2) (2.0.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->detectron2) (3.4.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.0)\n","/content/fast-labeling-workflow\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SsnPpp0nV7m6"},"source":["# Speed up your image segmentation workflow with model-assisted labeling\n","\n","A large dataset of labeled images is the first thing you need in any serious computer vision project.\n","Building such datasets is a time-consuming endeavour, involving lots of manual labeling work. This is especially true for tasks like image segmentation where the labels need to be very precise.\n","\n","One way to drastically speed up image labeling is by leveraging your machine learning models from the start. \n","Instead of labeling the entire dataset manually, you can use your model to help you by iterating between image labeling and model training.\n","\n","This tutorial will show you how to achieve such a fast labeling workflow for image segmentation with Segments.ai."]},{"cell_type":"markdown","metadata":{"id":"Re3TcGVfV7m7"},"source":["![A fast labeling workflow](https://github.com/segments-ai/fast-labeling-workflow/blob/master/img/fast-labeling-workflow-diagram.png?raw=1)"]},{"cell_type":"markdown","metadata":{"id":"-KMQ1chiV7m7"},"source":["[Segments.ai](https://segments.ai) is a labeling platform with powerful automation tools for image segmentation. \n","It also features a flexible API and Python SDK, which enable you to quickly set up custom workflows by uploading images and labels directly from your code.\n","\n","We will walk you through a simple but efficient setup:\n","\n","1. Upload your images to Segments.ai, and label a small subset.\n","2. Train a segmentation model on the labeled images.\n","3. Generate label predictions on the remaining images and upload them.\n","4. Correct the mistakes.\n","\n","You can find all code for this tutorial on [Github](https://github.com/segments-ai/fast-labeling-workflow), or follow along on [Google Colab](https://colab.research.google.com/github/segments-ai/fast-labeling-workflow/blob/master/demo.ipynb)."]},{"cell_type":"markdown","metadata":{"id":"eZqt6QRZV7m7"},"source":["## 1. Upload your images and label a small subset\n","\n","First, we need some images to label.\n","\n","If you have a folder of images on your pc, you can simply upload them to Segments.ai through the web interface: first create a new dataset, then upload the samples.\n","\n","But let's assume your data is in the cloud, and all you have is a list of image URLs. In this case, you can upload them to Segments.ai using our API or Python SDK. You need an API key for this, which can be created on your [account page](https://segments.ai/account).\n","\n","In this tutorial, our goal is to label a dataset of 100 tomato images. First, we upload the images using the Python SDK:"]},{"cell_type":"code","metadata":{"id":"CGnSvxywV7m8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608680798566,"user_tz":-60,"elapsed":1447,"user":{"displayName":"Nico","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0Bb0bACIyv9fqV3zWzGefWKcxTlKtkEba4qlPeA=s64","userId":"17943449425803794492"}},"outputId":"10d11803-d702-4899-d44b-865797e40ac4"},"source":["from segments import SegmentsClient # Install this package with 'pip install segments-ai'\n","from utils import get_image_urls\n","\n","# Set up the client\n","client = SegmentsClient('1d506cc26a24a7e68c64e9600416cd0529adf6a9')\n","dataset_name = 'kailkuhn/playground' # Name of a dataset you've created on Segments.ai\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["** fvcore version of PathManager will be deprecated soon. **\n","** Please migrate to the version in iopath repo. **\n","https://github.com/facebookresearch/iopath \n","\n","** fvcore version of PathManager will be deprecated soon. **\n","** Please migrate to the version in iopath repo. **\n","https://github.com/facebookresearch/iopath \n","\n"],"name":"stderr"},{"output_type":"stream","text":["Initialized successfully.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E95D17b8V7m8"},"source":["Once the images are uploaded, click the \"Start labeling\" button on the samples tab of your dataset and get to work! Rather than immediately labeling the entire dataset, let's start out by labeling around 20 images.\n","\n","Segments.ai's deep learning fueled superpixel tool makes the labeling a breeze."]},{"cell_type":"markdown","metadata":{"id":"r1iPt-tcV7m8"},"source":["## 2. Train a segmentation model on the labeled images\n","\n","After you've labeled a few images, go to the releases tab of your dataset and create a new release, for example with the name \"v0.1\". A release is a snapshot of your dataset at a particular point in time.\n","\n","Through the Python SDK, we can now initialize a SegmentsDataset from this release and visualize the labeled images. The SegmentsDataset is compatible with popular frameworks like PyTorch, Tensorflow and Keras."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"4ZdUXxEoV7m9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608680817317,"user_tz":-60,"elapsed":17219,"user":{"displayName":"Nico","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0Bb0bACIyv9fqV3zWzGefWKcxTlKtkEba4qlPeA=s64","userId":"17943449425803794492"}},"outputId":"54c73116-e820-43a6-cef5-0e09329b9caf"},"source":["from segments import SegmentsDataset\n","from utils import visualize, train_model\n","\n","# Initialize a dataset from the release file\n","release = client.get_release(dataset_name, 'v0.6')\n","dataset = SegmentsDataset(release, task='ground-truth', filter_by=['labeled'])\n","\n","# Visualize a few samples in the dataset\n","#for sample in dataset:\n","#    print(sample['name'])    \n","#    visualize(sample['image'], sample['segmentation_bitmap']) "],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/124 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Initializing dataset. This may take a few seconds...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 124/124 [00:14<00:00,  8.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["Initialized dataset with 124 images.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G0OfrAmvmuDU","executionInfo":{"status":"ok","timestamp":1608680823212,"user_tz":-60,"elapsed":585,"user":{"displayName":"Nico","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0Bb0bACIyv9fqV3zWzGefWKcxTlKtkEba4qlPeA=s64","userId":"17943449425803794492"}},"outputId":"3d333bb3-7cd8-44b9-c52e-06f052a9f932"},"source":["len(dataset)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["124"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"Bbw2j8duV7m9"},"source":["Next, let's train a computer vision model on the labeled images. Here we use Facebook's Detectron2 framework to train the model, but you can just as easily plug in your own custom models and training code."]},{"cell_type":"code","metadata":{"id":"9CvaDQMzV7m9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608680965974,"user_tz":-60,"elapsed":140712,"user":{"displayName":"Nico","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0Bb0bACIyv9fqV3zWzGefWKcxTlKtkEba4qlPeA=s64","userId":"17943449425803794492"}},"outputId":"4844d23f-7ed7-4052-91ac-748c0c349cba"},"source":["# Train an instance segmentation model on the dataset\n","from utils import train_model\n","model = train_model(dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 124/124 [00:08<00:00, 14.34it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Exported to 7a689518-0b6d-4284-b13e-9a9e8db39ee8_coco.json.\n","Metadata(evaluator_type='coco', image_root='segments/kailkuhn_playground/v0.6', json_file='7a689518-0b6d-4284-b13e-9a9e8db39ee8_coco.json', name='my_dataset', thing_classes=['leaf'])\n","\u001b[32m[12/22 23:47:19 d2.engine.defaults]: \u001b[0mModel:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n","    )\n","    (mask_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (mask_head): MaskRCNNConvUpsampleHead(\n","      (mask_fcn1): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn2): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn3): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (mask_fcn4): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n","      (deconv_relu): ReLU()\n","      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n",")\n","\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/22 23:47:19 d2.data.datasets.coco]: \u001b[0m\n","Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n","\n","\u001b[32m[12/22 23:47:19 d2.data.datasets.coco]: \u001b[0mLoaded 124 images in COCO format from 7a689518-0b6d-4284-b13e-9a9e8db39ee8_coco.json\n","\u001b[32m[12/22 23:47:19 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 124 images left.\n","\u001b[32m[12/22 23:47:19 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n","\u001b[36m|  category  | #instances   |\n","|:----------:|:-------------|\n","|    leaf    | 1787         |\n","|            |              |\u001b[0m\n","\u001b[32m[12/22 23:47:19 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n","\u001b[32m[12/22 23:47:19 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n","\u001b[32m[12/22 23:47:19 d2.data.common]: \u001b[0mSerializing 124 elements to byte tensors and concatenating them all ...\n","\u001b[32m[12/22 23:47:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.59 MiB\n"],"name":"stdout"},{"output_type":"stream","text":["Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[12/22 23:47:21 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/detectron2/data/detection_utils.py:414: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n","  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n","/usr/local/lib/python3.6/dist-packages/detectron2/data/detection_utils.py:414: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n","  torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n","/usr/local/lib/python3.6/dist-packages/detectron2/modeling/roi_heads/fast_rcnn.py:217: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n","  num_fg = fg_inds.nonzero().numel()\n"],"name":"stderr"},{"output_type":"stream","text":["\u001b[32m[12/22 23:47:29 d2.utils.events]: \u001b[0m eta: 0:01:51  iter: 19  total_loss: 3.162  loss_cls: 0.6788  loss_box_reg: 0.839  loss_mask: 0.6974  loss_rpn_cls: 0.8333  loss_rpn_loc: 0.08664  time: 0.3932  data_time: 0.0287  lr: 4.9953e-06  max_mem: 2432M\n","\u001b[32m[12/22 23:47:37 d2.utils.events]: \u001b[0m eta: 0:01:42  iter: 39  total_loss: 2.898  loss_cls: 0.6632  loss_box_reg: 0.829  loss_mask: 0.6922  loss_rpn_cls: 0.636  loss_rpn_loc: 0.07994  time: 0.3902  data_time: 0.0110  lr: 9.9902e-06  max_mem: 2432M\n","\u001b[32m[12/22 23:47:45 d2.utils.events]: \u001b[0m eta: 0:01:34  iter: 59  total_loss: 2.59  loss_cls: 0.6192  loss_box_reg: 0.8915  loss_mask: 0.6774  loss_rpn_cls: 0.3207  loss_rpn_loc: 0.07049  time: 0.3909  data_time: 0.0134  lr: 1.4985e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:47:53 d2.utils.events]: \u001b[0m eta: 0:01:26  iter: 79  total_loss: 2.348  loss_cls: 0.5701  loss_box_reg: 0.892  loss_mask: 0.6531  loss_rpn_cls: 0.1693  loss_rpn_loc: 0.0663  time: 0.3919  data_time: 0.0090  lr: 1.998e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:48:01 d2.utils.events]: \u001b[0m eta: 0:01:18  iter: 99  total_loss: 2.206  loss_cls: 0.5393  loss_box_reg: 0.9113  loss_mask: 0.6159  loss_rpn_cls: 0.08117  loss_rpn_loc: 0.06137  time: 0.3921  data_time: 0.0124  lr: 2.4975e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:48:09 d2.utils.events]: \u001b[0m eta: 0:01:11  iter: 119  total_loss: 2.076  loss_cls: 0.5088  loss_box_reg: 0.9008  loss_mask: 0.5737  loss_rpn_cls: 0.05831  loss_rpn_loc: 0.05714  time: 0.3942  data_time: 0.0106  lr: 2.997e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:48:17 d2.utils.events]: \u001b[0m eta: 0:01:03  iter: 139  total_loss: 2.028  loss_cls: 0.4943  loss_box_reg: 0.8828  loss_mask: 0.5329  loss_rpn_cls: 0.06481  loss_rpn_loc: 0.06513  time: 0.3956  data_time: 0.0109  lr: 3.4965e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:48:25 d2.utils.events]: \u001b[0m eta: 0:00:55  iter: 159  total_loss: 1.935  loss_cls: 0.4726  loss_box_reg: 0.8775  loss_mask: 0.4869  loss_rpn_cls: 0.04399  loss_rpn_loc: 0.05613  time: 0.3970  data_time: 0.0117  lr: 3.996e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:48:33 d2.utils.events]: \u001b[0m eta: 0:00:47  iter: 179  total_loss: 1.863  loss_cls: 0.446  loss_box_reg: 0.8317  loss_mask: 0.4417  loss_rpn_cls: 0.05243  loss_rpn_loc: 0.0563  time: 0.3975  data_time: 0.0116  lr: 4.4955e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:48:41 d2.utils.events]: \u001b[0m eta: 0:00:39  iter: 199  total_loss: 1.754  loss_cls: 0.4198  loss_box_reg: 0.8267  loss_mask: 0.4017  loss_rpn_cls: 0.03239  loss_rpn_loc: 0.05089  time: 0.3969  data_time: 0.0118  lr: 4.995e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:48:49 d2.utils.events]: \u001b[0m eta: 0:00:31  iter: 219  total_loss: 1.621  loss_cls: 0.3938  loss_box_reg: 0.8  loss_mask: 0.3553  loss_rpn_cls: 0.04013  loss_rpn_loc: 0.05423  time: 0.3979  data_time: 0.0108  lr: 5.4945e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:48:57 d2.utils.events]: \u001b[0m eta: 0:00:24  iter: 239  total_loss: 1.556  loss_cls: 0.3774  loss_box_reg: 0.7973  loss_mask: 0.3163  loss_rpn_cls: 0.0334  loss_rpn_loc: 0.04329  time: 0.3987  data_time: 0.0138  lr: 5.994e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:49:06 d2.utils.events]: \u001b[0m eta: 0:00:16  iter: 259  total_loss: 1.486  loss_cls: 0.3531  loss_box_reg: 0.7523  loss_mask: 0.2804  loss_rpn_cls: 0.04381  loss_rpn_loc: 0.05255  time: 0.4007  data_time: 0.0105  lr: 6.4935e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:49:14 d2.utils.events]: \u001b[0m eta: 0:00:08  iter: 279  total_loss: 1.441  loss_cls: 0.328  loss_box_reg: 0.7178  loss_mask: 0.2767  loss_rpn_cls: 0.02823  loss_rpn_loc: 0.05201  time: 0.4014  data_time: 0.0115  lr: 6.993e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:49:24 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 299  total_loss: 1.262  loss_cls: 0.2956  loss_box_reg: 0.6389  loss_mask: 0.2372  loss_rpn_cls: 0.03769  loss_rpn_loc: 0.04593  time: 0.4014  data_time: 0.0129  lr: 7.4925e-05  max_mem: 2558M\n","\u001b[32m[12/22 23:49:24 d2.engine.hooks]: \u001b[0mOverall training speed: 298 iterations in 0:01:59 (0.4014 s / it)\n","\u001b[32m[12/22 23:49:24 d2.engine.hooks]: \u001b[0mTotal training time: 0:02:01 (0:00:02 on hooks)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YaJb_ThJV7m9"},"source":["## 3. Generate and upload label predictions for the unlabeled images\n","\n","When the model is trained, we can run it on the unlabeled images to generate label predictions, and upload these predictions to Segments.ai:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"UYFogg2yV7m-","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1V0OPaFjn7jHuNuFFsqYb8_bEWTEOr11F"},"executionInfo":{"status":"ok","timestamp":1608681381653,"user_tz":-60,"elapsed":212592,"user":{"displayName":"Nico","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh0Bb0bACIyv9fqV3zWzGefWKcxTlKtkEba4qlPeA=s64","userId":"17943449425803794492"}},"outputId":"bb11e428-8ff0-4e1e-b0da-2c92b9eea082"},"source":["from segments.utils import bitmap2file\n","\n","# Initialize a new dataset, this time containing only unlabeled images\n","dataset = SegmentsDataset(release, task='ground-truth', filter_by='prelabeled')\n","\n","for sample in dataset:\n","    # Generate label predictions\n","    image = sample['image']\n","    segmentation_bitmap, annotations = model(image)\n","    \n","    # Visualize the predictions\n","    visualize(image, segmentation_bitmap)\n","    print(annotations)\n","    \n","    # Upload the predictions to Segments.ai\n","    file = bitmap2file(segmentation_bitmap)\n","    asset = client.upload_asset(file, 'label.png')    \n","    attributes = {\n","        'format_version': '0.1',\n","        'annotations': annotations,\n","        'segmentation_bitmap': { 'url': asset['url'] },\n","    }\n","    client.add_label(sample['uuid'], 'ground-truth', attributes, label_status='PRELABELED')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"PThaGnyMV7m-"},"source":["## 4. Verify and correct the predicted labels\n","\n","Now go back to Segments.ai and click the \"Start labeling\" button again to continue labeling. This time, your job is quite a bit easier: instead of having to label each image from scratch, you can simply correct the few mistakes your model made!\n","\n","The superpixel technology makes it very easy to correct the mistakes, and is a real time-saver here."]},{"cell_type":"markdown","metadata":{"id":"Ig8SzvVCV7m-"},"source":["## Next steps\n","\n","As you keep iterating between model training and labeling in this manner, your model will quickly get better and better. You'll reach a point where you're mostly just verifying the model's predictions, only having to correct the occasional mistakes on hard edge cases.\n","\n","Was this useful for you? Let us know! Make sure to check out the Segments.ai [documentation](https://docs.segments.ai/python-sdk) and don't hesitate to [contact us](https://segments.ai/contact) if you have any questions."]}]}